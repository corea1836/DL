import pandas as pd

# 모델에 데이터를 집어넣기 전에..

# <데이터 읽어오기(csv파일)>
data = pd.read_csv('gpascore.csv')
# print(data)
# csv = 엑셀과 같이 행과 열로 구조화된 데이터를 pandas.read_csv(')로 읽을 수 있다.
'''
     admit    gre   gpa  rank
0        0  380.0  3.21     3
1        1  660.0  3.67     3
2        1  800.0  4.00     1
3        1  640.0  3.19     4
4        0  520.0  2.33     4
..     ...    ...   ...   ...
421      1  750.0  3.44     4
422      1  710.0  3.66     4
423      0  450.0  3.25     4
424      1  760.0  3.76     2
425      1  710.0  3.82     3

[426 rows x 4 columns]
'''
#------------------------------------------------------------------------------------------------------------------------------------------------------

# <데이터 전처리(preprocessing) 하기>
# Q : 전처리를 하는 이유? 
# A : data중 빵꾸(null)난 자료들이 있다. 이런 빈부분은 1. 평균값을 넣거나 2. 행을 삭제함으로써 null이 없는 데이터셋으로 전처리 한다.

# dataset에서 빈칸 확인하기(열 기준(카테코리별)으로 확인해준다.)
# print(data.isnull().sum())
'''
admit    0
gre      1 -> gre라는 열에 하나가 비어있다라는 것을 보여준다.
gpa      0
rank     0
dtype: int64
'''

# <빈칸 제거(NaN/빈칸이 있는 행을 제거해준다.)>
data = data.dropna()
#print(data.isnull().sum())
'''
admit    0
gre      0
gpa      0
rank     0
dtype: int64
'''
# data = data.fillna(숫자/문자) -> 빈칸을 ()에 넣은 값으로 채워줌(평균값으로 넣으면 유용하게 쓸 수 있다.)

# <유용한 pandas 사용법들>
# 필요한 열만 출력하기(data['열 이름'])
# print(data['gpa'])
'''
0      3.21
1      3.67
2      4.00
3      3.19
4      2.33
       ... 
421    3.44
422    3.66
423    3.25
424    3.76
425    3.82
'''

# 열 중 최소값, 최대값, 갯수
# print(data['gpa'].min())
# 2.12
# print(data['gre'].max())
# 800.0
# print(data['gre'].count())
# 425(이 열에 데이터가 몇 개 있는지? = 행이 425개)

# <x데이터(input) 만들기>
# 380, 3.21, 3이라는 데이터를 array로 만들어 준다. -> [380, 3.21, 3]
# 학습시킬 데이터는 한 행을 리스트로 각각 묶고 하나의 큰 리스크로 만들어 x데이터로 만든다. -> [ [380, 3.21, 3], [660, 3.67, 3], [데이터3], [데이터4], ... ]
x데이터  = []
for i, rows in data.iterrows():
  x데이터.append([ rows['gre'], rows['gpa'], rows['rank'] ])
# print(rows)
# iterrows()함수는 pandas에서 dataframe(pandas로 연 가로(행), 세로(열)가 있는 data를 말함)을 한 행씩 출력할 수 있다.
# iterrows()함수는 변수를 2개까지 생성할 수 있는데 위의 반복문을 기준으로 i에는 행 번호(0부터 시작), row에는 데이터들이 한 행씩 들어간다. 
'''
admit      1.00
gre      760.00
gpa        3.76
rank       2.00
Name: 424, dtype: float64
admit      1.00
gre      710.00
gpa        3.82
rank       3.00
Name: 425, dtype: float64
.
.
.
'''

# rows['gre'] -> 'gre' 세로 열에 있는 데이터
'''
340.0
.
.
.
750.0
710.0
450.0
760.0
710.0
'''

# [ rows['gre'], rows['gpa'], rows['rank'] ] -> 하나의 리스트에 한 행의 x데이터로 만들 필요한 값들을 넣는다. 이렇게 만든 리스트를 반복문을 통해  x데이터로 만들 리스트에 넣는다.
# x데이터.append([ rows['gre'], rows['gpa'], rows['rank'] ])
# x데이터 = [ [데이터1], [데이터2], ... ]
# print(x데이터)
# [ [380.0, 3.21, 3.0], [660.0, 3.67, 3.0], [800.0, 4.0, 1.0], ... ]

# <y데이터(result) 만들기>
# 정답 데이터를 array에 넣는다. -> [ 0, 1, 1, 정답4, 정답5, ...  ] (경우에 따라 정답 데이터도 리스트로 감싸야 할 경우도 있다.)
y데이터 = data['admit'].values
# data['열 이름'].values를 하면 지정한 열의 모든 데이터를 리스트에 담아준다.
# print(y데이터)
'''
[0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0
...
 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1]
'''

#------------------------------------------------------------------------------------------------------------------------------------------------------

import tensorflow as tf

model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(64, activation='tanh'),
  tf.keras.layers.Dense(128, activation='tanh'),
  tf.keras.layers.Dense(1, activation='sigmoid'),
  # 마지막 노드는 하나의 노드만 있는게 좋다.(결과창이니까, )
])

# <모델 정의>

# 앞으로는 딥러닝 모델을 tensorflow안의 keras 도구를 활용해 만든다.
# model = tf.keras.models.Sequential([  ])
# 딥러닝 모델 : 여러개의 레이어와 히든 레이어를 가중치가 있는 간선으로 연결후 최적의 w 값을 찾도록 tensorflow를 통해 시킨다.
# Sequential 함수를 사용하면 이러한 모델을 쉽게 만들 수 있다.
# Suquential([ 레이어1, -> 레이어 = tf.keras.layers.Dense(한 층에 있는 노드의 수, Activation Function), 를 통해 딥러닝 모델의 히든 레이어를 생성할 수 있다.
#                                                     Actication Function = sigmoid, tanh, relu, sofrmax 원하는거 넣기(tnah, relu 많이씀)
#                                                                           * sigmoid = 모든 값을 0 ~ 1 사이로 압축시켜준다.
#              레이어2, 
#              레이어3,
#               ...   ])
# 노드 갯수는 결과가 잘 나올때까지 실험적으로 파악하는 수 밖에 없다.(보통 2의 제곱수로 표현)
# 딥러닝에서 정수 예측은 어렵다. 실수가 편하다.(굳이 정수로 하려면 round같은걸 써야함.)

#------------------------------------------------------------------------------------------------------------------------------------------------------

# <모델 compile>

# model.compile(optimizer=??, loss=??, metrics=['accuracy'])
# 모델에 compile을 해줘야 모델이 완성된다.

# model.complie에 들어갈 파라미터 
# optimizer : w값을 수정할 때 사용한다.(ex. 경사하강법 : 새로운 w1 <= w1 - a * dj()/dw1) optimizer는 빼는 값인 (a * dj()/dw1(기울기)) 이 값을 알맞게 조정해준다.
#             기울기를 뺄 때 매번 일정하게 빼면 학습이 잘 안될 수 있기 때문에 상황에 따라 learning rate를 조정하는 optimizer(ex. Adam, Momentum...)을 사용한다.
#             optimizer : adam, adagrad, adadelta, rmsprop, sgd ...
# loss : 문제마다 문제에 맞는 loss 함수를 사용하자. (결과가 0과 1 사이의 분류/확률 예측 : binary_crossentropy), Meansquarederror ...
# metrics : 딥러닝 모델을 어떤 요소로 평가할 것인지?

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# <모델 학습시키기>
# model.fit(x데이터, y데이터, epochs=10)

# model.fit에 들어갈 파라미터
# x데이터 : 학습 데이터(정답 예측에 필요한 input)
# y데이터 : 실제 정답  
# epochs : 파라미터로 넣은 데이터들을 몇 번 학습시킬 것인지(ex. epochs = 10이라고 저장하면 전체 데이터셋을 10번 학습한다.)
# model.fit( )에 데이터를 집어 넣을 때 파이썬 리스트를 그대로 넣을 수 없다. numpy array 혹은 tftensor를 집어넣어야 한다.


#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# <일반 리스트를 numpy array로 변환>
import numpy as np
# 왠만하면 import는 위에서 다 하자.
# numpy란? 파이썬으로 행렬, 백터(다차원 리스트, 행렬)등을 만들 때 사용하는 라이브러리

# np.array(데이터)
# numpy array로 변환해도 리스트와 생긴거나 조작은 다를게 없다. 하지만 model.fit()에 넣으려면 numpy array이어야 하기 때문에 변환한다.

model.fit( np.array(x데이터), np.array(y데이터), epochs=1000)
'''
Epoch 1/10
14/14 [==============================] - 0s 960us/step - loss: 0.7760 - accuracy: 0.4659
Epoch 2/10
14/14 [==============================] - 0s 797us/step - loss: 0.6940 - accuracy: 0.5435
...
Epoch 10/10
14/14 [==============================] - 0s 753us/step - loss: 0.6945 - accuracy: 0.5271
'''

'''
결과 해석
Epoch 10/10 -> Epoch : 전체 dataset을 통해 학습, 지정한 학습 횟수 중 실행한 횟수
14/14 [==============================] - 0s 753us/step - loss: 0.6945 - accuracy: 0.5271
0s .. : 걸린 시간
loss : 예측값과 실제값의 차이의 총 합(학습할수록 손실이 적어지면 학습이 잘 되고 있는 것)
accuracy : 예측한 값이 실제 데이터와 얼마나 잘 맞는지 평가한 것(높을수록 좋다.) 데이터가 얼마 없고 학습량도 적어 수치가 떨어지는 모습을 보이고 있다.(2 : 0.5435, 10 : 0.5271) -> 학습을 많이 시키자.

Epoch 1000/1000
14/14 [==============================] - 0s 816us/step - loss: 0.5180 - accuracy: 0.7459
'''

#----------------------------------------------------------------------------------------------------------------------------------------------------------------

# 학습시킨 모델로 예측해보기
# 위에서 한 학습으로 최적의 w값들을 찾았다.
# ex) GRE : 700, GPA : 3.7, Rank : 4 인 경우 합격 확률은?

# model.predict( x데이터 ) -> 아까 만든 model로 y값을 예측해주세요. 예측할 x데이터만 넣으면 된다.
예측값 = model.predict( [ [750, 3.70, 3], [400, 2.2, 1] ] )
print(예측값)
'''
* 성능 향상이 일어나지 않으면 다시 돌리면 된다.
  올바르게 했다면 딥러닝을 여러번 돌리고 가장 좋은 값을 저장하면 된다.
  epoch수를 늘리면 결과가 더욱 균일하게 나올 수 있다.

Epoch 1000/1000
14/14 [==============================] - 0s 762us/step - loss: 0.5306 - accuracy: 0.7506

[
  [0.9210422 ] -> 91%의 확률로 붙을거다.
  [0.05419171] -> 5%의 확률로 불을거다.
 ]
'''

'''
딥러닝 기초
1. 모델 만들기
2. 데이터 집어넣고 학습
3. 새로운 데이터 예측

딥러닝 성능 향상법
1. 데이터 전처리를 잘하면 성능이 매우 향상된다.
2. 파라미터 튜닝

딥러닝 성능향상은 많은 연구와 실험을 통해 판단해야한다.
'''